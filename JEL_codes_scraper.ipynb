{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "---------------------------------------\n",
    "# Using the American Economic Association's EconLit database (provided through EBSCOHost),\n",
    "# this script scrapes metadata on all economics journal articles back to a given year.\n",
    "# I have two plans for this data\n",
    "# 1) \n",
    "---------------------------------------\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's what I'm going to need:\n",
    "\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sqlite3 import Error\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\240-370-8956\\AppData\\Roaming\\Python\\Python37\\site-packages\\selenium\\webdriver\\phantomjs\\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead\n",
      "  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'UMD REQUIRES DUAL AUTHENTICATION AT THIS POINT; NO WAY AROUND THAT. ONLY HAS TO BE DONE ONCE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Initialize the browser and open the first page'''\n",
    "\n",
    "\n",
    "sr_browser = webdriver.PhantomJS(executable_path = \"C:/Users/240-370-8956/Desktop/STK/phantomjs-2.1.1-windows/phantomjs-2.1.1-windows/bin/phantomjs.exe\")\n",
    "sr_browser.set_window_size(1920, 1080)\n",
    "\n",
    "'''\n",
    "---------------------------------------\n",
    "# This next line uses a test search of all journal articles from 2019-01-01 to 2019-12-31\n",
    "# On EconLit, design your search (you can use SQL or their interface), and run it \n",
    "# Then, on the share tab, get a shareable link and start there\n",
    "# I can't use the URL of the search because UMD requires dual authentication.\n",
    "# If your university doesn't, you can go with the URL of the search\n",
    "---------------------------------------\n",
    "'''\n",
    "\n",
    "sr_browser.get(\"http://search.ebscohost.com.proxy-um.researchport.umd.edu/login.aspx?direct=true&db=ecn&bquery=&cli0=DT1&clv0=201901-201912&cli1=PT50&clv1=Journal+Article&type=1&searchMode=And&site=ehost-live\")\n",
    "\n",
    "username = sr_browser.find_element_by_id(\"username\")\n",
    "password = sr_browser.find_element_by_id(\"password\")\n",
    "\n",
    "\n",
    "'''Have to remember to redact these before I post on GitHub'''\n",
    "username.send_keys(\"jablevin\")\n",
    "password.send_keys(\"MehUMjkd.2\")\n",
    "\n",
    "sr_browser.find_element_by_name(\"_eventId_proceed\").click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''UMD REQUIRES DUAL AUTHENTICATION AT THIS POINT; NO WAY AROUND THAT. ONLY HAS TO BE DONE ONCE'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finds and selects the first result from the targeted search'''\n",
    "\n",
    "sr_browser.find_element_by_name(\"Result_1\").click()\n",
    "\n",
    "# Test 1\n",
    "#soup1 = BeautifulSoup(sr_browser.page_source)\n",
    "\n",
    "#print(soup3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is going to find and click the \"Next Page\" button\n",
    "\n",
    "def load_next_page():\n",
    "    try:\n",
    "        next_btn = sr_browser.find_element_by_id(\n",
    "            'ctl00_ctl00_MainContentArea_MainContentArea_topNavControl_btnNext'\n",
    "            )\n",
    "        next_btn.click()\n",
    "    except NoSuchElementException:\n",
    "        if sr_browser.current_url == 'about:blank':\n",
    "            print('Please check your internet connection.')\n",
    "        else:\n",
    "            sr_browser.save_screenshot('error.png')\n",
    "            print('The page is not as expected, screenshot saved.')\n",
    "            print('url: {}'.format(sr_browser.current_url))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe to store the article metadata in\n",
    "\n",
    "articles = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# The following while loop parses the articles page using BeautifulSoup, gets what I need, and puts in in a dataframe\n",
    "# This runs at about 3 seconds per page, and because there are 30-50k articles per year, that's not really sustainable\n",
    "# Most of the slowdown is the load_next_page function, I suspect because my wifi sucks and because PhantomJS isn't meant for this\n",
    "# Still, at one second per page (optimistic), this will take 11 hours per year. So don't start this unless you got time\n",
    "# ---------------------------------------\n",
    "\n",
    "n = 10\n",
    "\n",
    "\n",
    "while n > 0: \n",
    "    # within an EBSCOHost page, pulls HTML page source using BeautifulSoup \n",
    "    soup = BeautifulSoup(sr_browser.page_source)\n",
    "\n",
    "    dt_tags = soup.find_all('dt', attrs = {'data-auto': 'citation_field_label'})\n",
    "    field_names = [tag.string[:-1] for tag in dt_tags]\n",
    "\n",
    "    dd_tags = soup.find_all('dd', attrs = {'data-auto': 'citation_field_value'})\n",
    "    field_values = [tag.text for tag in dd_tags]\n",
    "\n",
    "    pagedict = dict(zip(field_names, field_values))\n",
    "\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # These are the fields I need:\n",
    "    # Accession number is a unique EconLit identifier\n",
    "    # Author is a field with a list of authors separated by semicolons\n",
    "    # Author Affiliation is the affiliation of the above authors, respectively; also semicolon separated\n",
    "    # Descriptors is the JEL codes associated with the paper, semicolon seperated\n",
    "    # Keywords are decided by the authors; also semicolon separated\n",
    "    # ---------------------------------------\n",
    "    needed_fields = ['Accession Number', 'Author', 'Author Affiliation', 'Source', 'Title', 'Descriptors', 'Keywords']\n",
    "    for field in needed_fields:\n",
    "        if field not in pagedict.keys():\n",
    "            pagedict[field] = ''\n",
    "\n",
    "    # Recognizes the relevant characters in the source code to grab them\n",
    "    re_pattern = r'\\([A-Z]\\d\\d\\)'\n",
    "    descriptors_list = [item[1:-1] for item in re.findall(re_pattern, pagedict['Descriptors'])]\n",
    "    descriptors = '; '.join(descriptors_list)\n",
    "    keywords = ' '.join(pagedict['Keywords'].split())\n",
    "\n",
    "    finaldict = {'an': pagedict['Accession Number'],\n",
    "                 'author': pagedict['Author'],\n",
    "                 'aff': pagedict['Author Affiliation'],\n",
    "                 'source': pagedict['Source'],\n",
    "                 'title': pagedict['Title'],\n",
    "                 'jel': descriptors,\n",
    "                 'keywords': keywords\n",
    "                 }\n",
    "\n",
    "    #print(finaldict)\n",
    "    \n",
    "    #add finaldict to dataframe \n",
    "    articles = articles.append(pd.DataFrame(finaldict, index = [0]), ignore_index = True)\n",
    "\n",
    "    # Go to the next page \n",
    "    load_next_page()\n",
    "    \n",
    "    \n",
    "    n -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports the dataframe to CSV for analysis in Stata\n",
    "\n",
    "articles.to_csv(r'C:\\Users\\240-370-8956\\Dropbox\\Research\\JEL_codes\\Data\\articles.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
